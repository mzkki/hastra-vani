{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, models\n",
    "import itertools\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direktori dataset\n",
    "VIDEO_TRAIN_DIR = '../data/videos/train/'\n",
    "VIDEO_VAL_DIR = '../data/videos/validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract frames from video\n",
    "def extract_frames_from_video(video_path, num_frames=30, target_size=(224, 224)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_interval = frame_count // num_frames\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, target_size)\n",
    "            frame = frame / 255.0\n",
    "            frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all video files and labels based on subfolder\n",
    "def get_video_files_and_labels(video_dir):\n",
    "    video_files = []\n",
    "    labels = {}\n",
    "    for subfolder in os.listdir(video_dir):\n",
    "        subfolder_path = os.path.join(video_dir, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                if file.endswith(\".mp4\"):\n",
    "                    video_path = os.path.join(subfolder_path, file)\n",
    "                    video_files.append(video_path)\n",
    "                    labels[video_path] = subfolder\n",
    "    return video_files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate batches of video data with labels\n",
    "def video_data_generator(video_dir, labels, batch_size=16, num_frames=30, target_size=(224, 224)):\n",
    "    video_files, _ = get_video_files_and_labels(video_dir)\n",
    "    while True:\n",
    "        batch_videos = []\n",
    "        batch_labels = []\n",
    "        for video_path in itertools.islice(video_files, 0, batch_size):\n",
    "            frames = extract_frames_from_video(video_path, num_frames, target_size)\n",
    "            label = labels.get(video_path)\n",
    "            batch_videos.append(frames)\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        batch_videos = np.array(batch_videos)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        yield batch_videos, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator and validation generator prepared.\n",
      "Jumlah video pelatihan: 52\n",
      "Jumlah video validasi: 52\n"
     ]
    }
   ],
   "source": [
    "# Get all video files and labels for training and validation\n",
    "video_train_files, labels_train = get_video_files_and_labels(VIDEO_TRAIN_DIR)\n",
    "video_val_files, labels_val = get_video_files_and_labels(VIDEO_VAL_DIR)\n",
    "\n",
    "# Initialize LabelEncoder to convert string labels to integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the training labels and transform labels\n",
    "train_labels_encoded = label_encoder.fit_transform(list(labels_train.values()))\n",
    "val_labels_encoded = label_encoder.transform(list(labels_val.values()))\n",
    "\n",
    "# Map the labels back to their respective video files\n",
    "labels_train_encoded = {video_file: train_labels_encoded[i] for i, video_file in enumerate(video_train_files)}\n",
    "labels_val_encoded = {video_file: val_labels_encoded[i] for i, video_file in enumerate(video_val_files)}\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = len(video_train_files) // 32\n",
    "validation_steps = len(video_val_files) // 32\n",
    "\n",
    "# Create video data generators for training and validation\n",
    "train_video_generator = video_data_generator(VIDEO_TRAIN_DIR, labels_train_encoded, batch_size=16)\n",
    "validation_video_generator = video_data_generator(VIDEO_VAL_DIR, labels_val_encoded, batch_size=16)\n",
    "\n",
    "print(f\"Train generator and validation generator prepared.\")\n",
    "print(f\"Jumlah video pelatihan: {len(video_train_files)}\")\n",
    "print(f\"Jumlah video validasi: {len(video_val_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Dropout(0.5)  # Dropout layer with 50% probability\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "])\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN 3D model for sign language recognition\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(30, 224, 224, 3)),\n",
    "    layers.Conv3D(32, (3, 3, 3), activation='relu'),\n",
    "    layers.MaxPooling3D((2, 2, 2)),\n",
    "    layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    layers.MaxPooling3D((2, 2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)  # Lower learning rate\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 54s/step - accuracy: 0.0000e+00 - loss: 3.2643 - val_accuracy: 0.1250 - val_loss: 4.0727\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 34s/step - accuracy: 0.1250 - loss: 4.0727 - val_accuracy: 0.3750 - val_loss: 4.2594\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 32s/step - accuracy: 0.3750 - loss: 4.2594 - val_accuracy: 0.1875 - val_loss: 3.0400\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 30s/step - accuracy: 0.1875 - loss: 3.0400 - val_accuracy: 0.4375 - val_loss: 2.5409\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 31s/step - accuracy: 0.4375 - loss: 2.5409 - val_accuracy: 0.3750 - val_loss: 2.5287\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 35s/step - accuracy: 0.3750 - loss: 2.5287 - val_accuracy: 0.5625 - val_loss: 2.1370\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 31s/step - accuracy: 0.5625 - loss: 2.1370 - val_accuracy: 0.5625 - val_loss: 1.6404\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - accuracy: 0.5625 - loss: 1.6404 - val_accuracy: 0.8750 - val_loss: 1.2090\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 43s/step - accuracy: 0.8750 - loss: 1.2090 - val_accuracy: 0.8750 - val_loss: 1.0223\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 37s/step - accuracy: 0.8750 - loss: 1.0223 - val_accuracy: 0.8125 - val_loss: 0.7118\n"
     ]
    }
   ],
   "source": [
    "# Train the model with video data\n",
    "history = model.fit(\n",
    "    train_video_generator,\n",
    "    validation_data=validation_video_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('../models/models_video', exist_ok=True)\n",
    "\n",
    "# Now save the model\n",
    "model.save('../models/models/video_model.h5')\n",
    "\n",
    "# Menyimpan arsitektur model ke format JSON\n",
    "model_json = model.to_json()\n",
    "with open('../models/models/video_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 30, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model_path = '../models/models_video/video_model.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Print the input shape of the model\n",
    "print(model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for the video\n",
    "def preprocess_video(video_path, target_size=(224, 224), max_frames=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    count = 0\n",
    "    while cap.isOpened() and count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Pad or truncate the frames to exactly match the required number of frames (30 in this case)\n",
    "    if len(frames) < max_frames:\n",
    "        # If there are fewer than 30 frames, pad with empty frames\n",
    "        frames += [np.zeros_like(frames[0])] * (max_frames - len(frames))\n",
    "    frames = np.array(frames)\n",
    "\n",
    "    # Add batch dimension, making shape (1, 30, 224, 224, 3)\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed video shape: (1, 30, 224, 224, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Prediction: [[2.86035985e-02 3.13805610e-01 1.85759723e-01 7.16920570e-02\n",
      "  1.29557149e-02 1.10213503e-01 1.06744289e-01 1.19678535e-01\n",
      "  1.81934040e-04 1.80583569e-07 2.91786011e-04 4.00133540e-05\n",
      "  2.39438901e-04 2.41743913e-03 1.03944685e-05 1.49587882e-04\n",
      "  1.01852493e-05 4.20578499e-06 2.22209748e-03 1.62031240e-04\n",
      "  6.04126370e-04 4.03868034e-05 6.88902655e-05 3.16705591e-05\n",
      "  1.84414536e-03 4.22284752e-02]]\n",
      "Predicted label: B\n"
     ]
    }
   ],
   "source": [
    "# Define the class labels\n",
    "class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "# Path to your video\n",
    "video_path = '../data/videos/train/C/c1.mp4'\n",
    "\n",
    "# Preprocess the video\n",
    "preprocessed_video = preprocess_video(video_path)\n",
    "\n",
    "# Check the shape of the preprocessed video\n",
    "print(\"Preprocessed video shape:\", preprocessed_video.shape)\n",
    "\n",
    "# Predict using the model\n",
    "prediction = model.predict(preprocessed_video)\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "# Get the index of the class with the highest probability\n",
    "predicted_index = np.argmax(prediction)\n",
    "\n",
    "# Map the predicted index to the corresponding class label\n",
    "predicted_label = class_labels[predicted_index]\n",
    "\n",
    "# Print the predicted label\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
