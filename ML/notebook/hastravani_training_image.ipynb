{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TRAIN_DIR = '../data/image/train/'\n",
    "IMAGE_VALIDATION_DIR = '../data/image/validation/'\n",
    "VIDEO_TRAIN_DIR = '../data/videos/train/'\n",
    "VIDEO_VAL_DIR = '../data/videos/validation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing untuk Gambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation untuk gambar pelatihan\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalisasi gambar\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2851 images belonging to 26 classes.\n",
      "Found 2851 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "# Hanya normalisasi untuk data validasi\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    IMAGE_TRAIN_DIR,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,  # Anda bisa mencoba ukuran batch lebih kecil jika diperlukan\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    IMAGE_VALIDATION_DIR,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing untuk Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator and validation generator prepared.\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk mengambil frame dari video\n",
    "def extract_frames_from_video(video_path, num_frames=30, target_size=(224, 224)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_interval = frame_count // num_frames\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, target_size)\n",
    "            frame = frame / 255.0  # Normalisasi\n",
    "            frames.append(np.float32)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "\n",
    "# Fungsi untuk mendapatkan semua video dan label berdasarkan subfolder\n",
    "def get_video_files_and_labels(video_dir):\n",
    "    video_files = []\n",
    "    labels = {}\n",
    "    for subfolder in os.listdir(video_dir):\n",
    "        subfolder_path = os.path.join(video_dir, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # Dapatkan semua file video di dalam subfolder\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                if file.endswith(\".mp4\"):  # Pastikan hanya file .mp4 yang diproses\n",
    "                    video_path = os.path.join(subfolder_path, file)\n",
    "                    video_files.append(video_path)\n",
    "                    labels[video_path] = subfolder  # Gunakan nama subfolder sebagai label\n",
    "    return video_files, labels\n",
    "\n",
    "# Fungsi generator untuk video dengan label\n",
    "import itertools\n",
    "\n",
    "# Fungsi generator untuk video dengan label\n",
    "def video_data_generator(video_dir, labels, batch_size=16, num_frames=30, target_size=(224, 224)):\n",
    "    video_files, _ = get_video_files_and_labels(video_dir)\n",
    "    while True:\n",
    "        batch_videos = []\n",
    "        batch_labels = []\n",
    "        for video_path in itertools.islice(video_files, 0, batch_size):\n",
    "            frames = extract_frames_from_video(video_path, num_frames, target_size)\n",
    "            label = labels.get(video_path)  # Ambil label berdasarkan path video\n",
    "            batch_videos.append(frames)\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        batch_videos = np.array(batch_videos)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        yield batch_videos, batch_labels  # Mengembalikan tuple (data, label)\n",
    "\n",
    "\n",
    "# Membaca video dari direktori pelatihan dan validasi\n",
    "VIDEO_TRAIN_DIR = '../data/videos/train/'\n",
    "VIDEO_VAL_DIR = '../data/videos/validation/'  # Pastikan Anda memiliki direktori validasi\n",
    "\n",
    "# Dapatkan semua file video dan label berdasarkan subfolder\n",
    "video_train_files, labels_train = get_video_files_and_labels(VIDEO_TRAIN_DIR)\n",
    "video_val_files, labels_val = get_video_files_and_labels(VIDEO_VAL_DIR)\n",
    "\n",
    "# Menghitung langkah per epoch dan langkah validasi\n",
    "steps_per_epoch = len(video_train_files) // 32  # batch_size = 32\n",
    "validation_steps = len(video_val_files) // 32  # batch_size = 32\n",
    "\n",
    "# Buat generator untuk pelatihan dan validasi\n",
    "train_video_generator = video_data_generator(VIDEO_TRAIN_DIR, labels_train, batch_size=16)\n",
    "validation_video_generator = video_data_generator(VIDEO_VAL_DIR, labels_val, batch_size=16)\n",
    "\n",
    "\n",
    "print(f\"Train generator and validation generator prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membangun Model CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - accuracy: 0.1693 - loss: 3.0095 - val_accuracy: 0.7611 - val_loss: 0.9157\n",
      "Epoch 2/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - accuracy: 0.8067 - loss: 0.7507 - val_accuracy: 0.9197 - val_loss: 0.3342\n",
      "Epoch 3/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - accuracy: 0.9403 - loss: 0.2306 - val_accuracy: 0.9835 - val_loss: 0.0928\n",
      "Epoch 4/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - accuracy: 0.9782 - loss: 0.0765 - val_accuracy: 0.9909 - val_loss: 0.0583\n",
      "Epoch 5/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - accuracy: 0.9903 - loss: 0.0412 - val_accuracy: 0.9982 - val_loss: 0.0131\n",
      "Epoch 6/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 1s/step - accuracy: 0.9944 - loss: 0.0205 - val_accuracy: 0.9930 - val_loss: 0.0261\n",
      "Epoch 7/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 1s/step - accuracy: 0.9935 - loss: 0.0346 - val_accuracy: 0.9989 - val_loss: 0.0055\n",
      "Epoch 8/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 1s/step - accuracy: 0.9983 - loss: 0.0086 - val_accuracy: 0.9996 - val_loss: 0.0012\n",
      "Epoch 9/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0090 - val_accuracy: 0.9975 - val_loss: 0.0119\n",
      "Epoch 10/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2s/step - accuracy: 0.9979 - loss: 0.0112 - val_accuracy: 0.9996 - val_loss: 0.0016\n"
     ]
    }
   ],
   "source": [
    "# Membangun model CNN untuk pengenalan gambar\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(224, 224, 3)),  # Menyatakan bentuk input secara eksplisit\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')  # Jumlah kelas\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Melatih model dengan data pelatihan\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 634ms/step - accuracy: 0.9998 - loss: 0.0013\n",
      "Validation Accuracy: 99.96%\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi model\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah video pelatihan: 52\n",
      "Jumlah video validasi: 52\n"
     ]
    }
   ],
   "source": [
    "print(f\"Jumlah video pelatihan: {len(video_train_files)}\")\n",
    "print(f\"Jumlah video validasi: {len(video_val_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Melatih model dengan data video\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_video_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_video_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\HASTRA_VANI\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\HASTRA_VANI\\myenv\\Lib\\site-packages\\optree\\ops.py:752\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    750\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    751\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: object"
     ]
    }
   ],
   "source": [
    "# Membangun model CNN 3D untuk pengenalan bahasa isyarat dalam video\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(30, 224, 224, 3)),  # 30 frame, resolusi 224x224, 3 saluran warna\n",
    "    layers.Conv3D(32, (3, 3, 3), activation='relu'),\n",
    "    layers.MaxPooling3D((2, 2, 2)),\n",
    "    layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    layers.MaxPooling3D((2, 2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(labels_train), activation='softmax')  # Jumlah kelas sesuai jumlah label\n",
    "])\n",
    "\n",
    "# Menyusun model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Melatih model dengan data video\n",
    "history = model.fit(\n",
    "    train_video_generator,\n",
    "    validation_data=validation_video_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
